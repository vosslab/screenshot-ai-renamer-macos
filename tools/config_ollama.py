#!/usr/bin/env python3

import os
import subprocess
import time

from ollama import chat

from tools import llm_wrapper

#============================================
def is_ollama_running():
	"""
	Checks if the Ollama server is running.
	"""
	try:
		output = subprocess.check_output(["pgrep", "ollama"], text=True).strip()
		if output:
			return True
	except subprocess.CalledProcessError:
		pass
	return False

#============================================
def start_ollama_if_needed():
	"""
	Checks if Ollama is running and provides instructions if it's not.
	"""
	if not is_ollama_running():
		print("\n⚠Ollama is not running! Start it with: `ollama serve`\n")
		print("Attempting to start Ollama automatically...")
		subprocess.Popen(["ollama", "serve"])
		time.sleep(3)  # Give it some time to start
		if is_ollama_running():
			print("Ollama started successfully!")
		else:
			print("Failed to start Ollama. Please run `ollama serve` manually.")

#============================================
def pull_ollama_model(model_name):
	"""
	Automatically pulls the required Ollama model if it is not available.
	"""
	print(f"Checking for model: {model_name}")
	try:
		models = llm_wrapper.list_ollama_models()
	except RuntimeError as exc:
		print(exc)
		raise

	if model_name not in models:
		print(f"Model {model_name} not found locally. Pulling now...")
		subprocess.run(["ollama", "pull", model_name], check=True)
		print(f"Successfully pulled model: {model_name}")


#============================================
# Select Ollama Model Based on VRAM (allow env override)
vram_size_gb = llm_wrapper.get_vram_size_in_gb()
MODEL_NAME = os.environ.get("OLLAMA_MODEL", "llama3.2:1b-instruct-q4_K_M")

if "OLLAMA_MODEL" not in os.environ and vram_size_gb:
	if vram_size_gb > 30:
		MODEL_NAME = "phi4:14b-q8_0"
	elif vram_size_gb > 14:
		MODEL_NAME = "phi4:14b-q4_K_M"
	elif vram_size_gb > 4:
		MODEL_NAME = "llama3.2:3b-instruct-q5_K_M"

print(f"Selected Ollama model: {MODEL_NAME}")

# Ensure Ollama is running
start_ollama_if_needed()
# Ensure the model is pulled
pull_ollama_model(MODEL_NAME)

#============================================
def unit_test():
	"""
	Simple unit test for LLM function.
	"""
	print("Running unit test...")
	import random
	# Generate a random math problem
	num1 = random.randint(10, 99)
	num2 = random.randint(10, 99)
	expected_answer = num1 + num2
	# Create a prompt
	prompt = f"What is {num1} + {num2}? "
	prompt += "Provide just the answer in plain text."
	# Get response from LLM
	response = run_ollama(prompt)
	# Attempt to parse the response
	try:
		ai_answer = int(response)
	except ValueError:
		print(f"FAILED: Response was not a valid number → {response}")
	if ai_answer == expected_answer:
		print(f"SUCCESS! {num1} + {num2} = {ai_answer}")
	else:
		print(f"FAILED: Expected {expected_answer}, but got {ai_answer}")

#============================================
def run_ollama(prompt: str, model: str = MODEL_NAME, max_retries: int = 2) -> str:
	"""
	Generate a response using the Ollama model.

	Args:
		prompt (str): The prompt to send to the model.
		model (str): The name of the Ollama model to use.

	Returns:
		str: The response content generated by the model.
	"""
	last_error = None
	for attempt in range(1, max_retries + 1):
		t0 = time.time()
		try:
			response = chat(
				model=model,
				messages=[{'role': 'user', 'content': prompt}]
			)
			response_content = response['message']['content'].strip()
			parsed = llm_wrapper.extract_response_text(response_content)
			if parsed:
				response_content = parsed
			print(f"Ollama completed in {time.time()-t0:.2f} seconds (attempt {attempt})")
			return response_content
		except Exception as exc:  # pylint: disable=broad-exception-caught
			last_error = exc
			print(f"Ollama call failed on attempt {attempt}: {exc}")
			time.sleep(attempt)

	raise RuntimeError(f"Ollama failed after {max_retries} attempts using model {model}") from last_error
